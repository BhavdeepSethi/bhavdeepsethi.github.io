<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

    <title><![CDATA[Category: tech | Deep Musings]]></title>
    <link href="http://www.deepmusings.com/blog/categories/tech/atom.xml" rel="self"/>
    <link href="http://www.deepmusings.com/"/>
    <updated>2015-08-16T01:44:54-07:00</updated>
    <id>http://www.deepmusings.com/</id>
    <author>
        <name><![CDATA[Bhavdeep Sethi]]></name>
        <email><![CDATA[bhavdeep_sethi@yahoo.in]]></email>
      </author>
    <generator uri="http://octopress.org/">Octopress</generator>

    
    <entry>
        <title type="html"><![CDATA[Apache Spark - Can it be lupus?]]></title>
        <link href="http://www.deepmusings.com/blog/2015/08/16/apache-spark-can-it-be-lupus/"/>
        <updated>2015-08-16T00:50:31-07:00</updated>
        <id>http://www.deepmusings.com/blog/2015/08/16/apache-spark-can-it-be-lupus</id>
        <content type="html"><![CDATA[<p>
Hmm&#8230;so you&#8217;re trying to diagnose problems in your Spark Job and can&#8217;t seem to make progress? I&#8217;ve been there. 
Working with Spark is similar to working with a Machine Learning algorithm with respect to the number of (hyper) parameters. 
Tuning them properly can be really hard initially. But, if you understand the semantics behind each of them, use it properly, you&#8217;ll see it&#8217;s true potential. I can&#8217;t stress on how much of an improvement it is over the standard Map Reduce paradigm.</p>


<p><br />
I&rsquo;ve played with Spark a lot over the last month. Here are some of the things I&rsquo;ve learned that may help you cure your ailing Spark Job.
<br /></p>

<!--more-->


<p>
Start by reading these: 
<ul>
    <li><a href="http://spark.apache.org/docs/latest/tuning.html" target="_blank">Tuning Spark</a> (You may want to change to the Spark Version currently deployed)
    <li><a href="http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/" target="_blank">Tune your Spark Jobs Part 1</a>
    <li><a href="http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/" target="_blank">Tune your Spark Jobs Part 2</a>
</ul>
<br />
<hr />
<h3>Not all Spark programs are created equal</h3>
<p>Spark has a lot of parameters that you can play with. Each can have quite an impact on the running time of your spark job. The four parameters that are probably the most critical are, <code> --num-executors, --driver-memory, --executor-memory and --executor-cores</code>.  Just increasing the num-executors does not mean your job will run faster. When you persist/cache any RDD, that is shared by all tasks running in an executor. So in most cases, increasing the parallelism (executor-cores) will have more effect as they can access the same cache. However, make sure you change executor-memory accordingly. The executor memory is shared among all the tasks. So if you have 16g and you can run 3 tasks in parallel, then each task will get around 5G.  If you notice that only a fraction of your RDD is getting cached, you need to increase the amount of partitions it can access. Increasing the number of executors and reducing the executor-cores might help with this. 
</p>
<br />
<hr />
<h3> Things to be aware of when you use variables.</h3>
<p>If you&#8217;re passing values via command line arguments to your Spark jobs, you need to be extra careful. The values that you pass are only accessible to the driver class. If you&#8217;re using Flags with default values, then the executors will receive the default values and not the ones you passed! For mandatory flags, it will just pass null. To avoid such cases, you can broadcast these flags and access them using the broadcast variables. </p>
<br />
<hr />
<h3>Kryo. Always.</h3>
<p>
Always always use Kryo. You can check the difference between Kryo and Java Serialization <a href="https://github.com/eishay/jvm-serializers/wiki" target="_blank">here</a>. When caching, it will use less far less space and will still be much faster. Seriously, don&#8217;t use java serialization. </p>
<br />
<hr />
<h3>StackOverflow problems? Cut the DAG.</h3>
<p>If you have long DAG of RDD objects, it needs to be serialized as part of the task creation. Serializing this leads to stackoverflow error. It&#8217;s generally common with algorithms where you perform iterations on the same RDD (eg. ALS). To fix these errors, you need to enable checkpointing (sparkContext.setCheckpointDir) and do a rdd.count() to cut the DAG. 
</p>
<br />
<hr />
<h3>Java heap space/OOM issues. </h3>
<p>
There are many variations of these errors. Typical ones are &#8220;Out of Java heap Space&#8221;, &#8220;OOM errors&#8221;. But, there are ones that are more subtle like &#8220;Failed without being ACK&#8217;d&#8221;, &#8220;Futures timed out after [n seconds]&#8221;.  These generally happen when the threads are busy with GC and couldn&#8217;t send the heartbeat message to the driver.  A good way to debug these are by going through the GC logs. <br />
<pre> spark.executor.extraJavaOptions=-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps </pre>
<br />
You can tweak your configuration based on the behavior seen by the gc logs. If there are lot of Full GC cycles happening, then you tuning config parameters like &#8220;spark.storage.memoryFraction&#8221; may help. For the timeout errors, changing the following defaults: <pre>spark.akka.frameSize=120</pre><pre>spark.rpc.askTimeout=120</pre></p>
<br />
<hr />
<h3>Everyday I&#8217;m shuffling.</h3>
When doing operations like joins, intersection, cartesian, etc. consider partitioning your RDD using a Hash Partitioner. In particular, if you&#8217;re joining a large RDD with a relatively smaller one, consider partitioning the larger RDD at the outset. This will ensure that only the elements of the smaller RDD are shuffled. </p>


<p><br /></p>

<hr />


<h3>Broadcast it! </h3>


<p>Broadcasting data can make your jobs really really fast. It obviously helps knowing what data size you&#8217;re dealing with. Assume you&#8217;re taking a cartesian between two RDDS, one of size 300k, other 100k. The resultant RDD would be 90 x 10<sup>9</sup>. That&#8217;s 90 billion pairs. The network shuffle required here will most likely kill your Spark job. However, if you broadcast the 100K RDD, creating the cartesian takes less than 5 minutes as it involves a simple flatMap operation over each element in it&#8217;s own partition. </p>


<p><br /></p>

<hr />


<h3>Prefer Enums, Integers over Strings</h3>


<p>Prefer using Enums, Integers over Strings and large objects since it affects serialization/shuffle times. If you need to use a large object simply as a key, or in cases where your algorithm demands integers, we can use zipWithIndex to create a object -> integer (with the reverse mapping) RDD and cache it. If it&#8217;s small enough, just create a BiMap and broadcast it. </p>




<p>
Spark is really really powerful. Use it properly, and you can solve most of your problems. 
Remember, it&#8217;s never lupus! 
</p>


<p><img src="http://www.deepmusings.com/assets/its-not-lupus.jpg" ></p>
]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Monolithic Repo vs Micro Repos]]></title>
        <link href="http://www.deepmusings.com/blog/2015/06/06/single-repo-vs-multiple-repos/"/>
        <updated>2015-06-06T22:21:51-07:00</updated>
        <id>http://www.deepmusings.com/blog/2015/06/06/single-repo-vs-multiple-repos</id>
        <content type="html"><![CDATA[<p>

Recently, I have been working with a single repo which had the entire codebase of the company. It was basically a heirachial maven project with each service being a subproject/module. Prior to this, I&#8217;ve always worked in an environment where each service had it&#8217;s own repository. I guess there are pros and cons of each method. 

Some of the points in favor of the single codebase approach I’ve heard are: 
<!--more -->
<ul>
    <li> You don’t have to worry about versioning. You end up having a single version for your product. This helps communications with QA, clients and managers. Since everyone has to pull in the same changes, everyone has access to the same thrift sources as well. 
    <li> Increased collaboration. Since, you have access to the entire codebase, you can look/learn/contribute to the code written by other teams as well. 
    <li> Reduces code duplication. Code discovery is much easier and reduces the amount of code duplicated across teams. 
</ul>
</p>


<p>However, there are problems with this approach as well:</p>

<ul>
    <li>Every time someone modifies code in a separate module, you have to recompile/generate sources to fix the compilation errors. Not to mention compiling takes a lot of time as well. 
    </li>
    <li>
    I&#8217;ve seen Intellij giving weird path errors even though everything is fine. A syncronize with re-importing the pom fixes this but, it&#8217;s still annoying.
    </li>
    <li>Unnecessary dependency. I&#8217;ve seen people continue using a particular framework because it&#8217;s available and the entire code base uses it. A lot of times people tend to force their solutions to the existing framework.  With Micro repos, each service can use the framework best suited for the job. 
    </li>
</ul>




<p>I personally prefer working with seperate repos. It&#8217;s a lot cleaner and for me, it symobilizes true service oriented architecture.Considering most of the time I&#8217;ll be developing on my local machine, I need that enviornment to be super fast. Having a good build tool probably makes the distinction between single vs micro repos even more obscure. 
</p>


<p>Yet, the monolithic repo seems to be the pre-dominant approach. I believe Google and Facebook do this. But the build enviornments in these companies are quite matured as well. So, what about the startups out there? Is there a preference?</p>

<p></p></p>

<p>
</p>

]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Interesting Papers from Cloud & Big Data]]></title>
        <link href="http://www.deepmusings.com/blog/2015/05/03/interesting-papers-from-cloud-and-big-data/"/>
        <updated>2015-05-03T08:13:07-07:00</updated>
        <id>http://www.deepmusings.com/blog/2015/05/03/interesting-papers-from-cloud-and-big-data</id>
        <content type="html"><![CDATA[<p>
One of courses that I took in Columbia, with a really good course <i><b>content</b></i>, was Cloud and Big Data. 
I took the course in Fall &#8216;14 and TA&#8217;ed it in Spring &#8216;15. 
Every week, two papers were released that the students were supposed to read and submit a short summary about. 
</p>


<p>
These are some really good papers and if you&#8217;re interested in Distributed Systems, you&#8217;ll really love reading them. 

I&#8217;m sharing the list here (in no particular order) and hope you enjoy reading them! 
</p>


<!--more-->


<p>
<h3> Cloud & Big Data Reading Paper List </h3>
</p>




<p>
<ul>
<li>
<b>The Google File System</b><br/>
Ghemawat, Sanjay;Gobioff, Howard; and Leung, Shun-Tak. ACM SIGOPS Operating Systems Review, 37(5) . 29-43.<br/>
<a href="http://static.googleusercontent.com/media/research.google.com/en/us/archive/gfs-sosp2003.pdf">GFS: Google File System</a><br />
</li>
<br /><br />
<li>
<b>CloudCmp: Comparing Public Cloud Providers</b><br />
Ang Li; Xiaowei Yang; Duke University, {angl, xwy}@cs.duke.edu; Srikanth Kandula; Ming Zhang; Microsoft Research {srikanth, mzh}@microsoft.com<br />
<a href="https://www.cs.duke.edu/~angl/papers/imc10-cloudcmp.pdf">CloudCmp: Comparing Public Cloud Providers</a><br />
</li>
<br /><br />
<li>
<b>Xen and the Art of Virtualization</b><br />
Barham, Paul;Dragovic, Boris;Fraser, Keir;Hand, Steven;Harris, Tim;Ho, Alex;Neugebauer, Rolf;Pratt, Ian; and Warfield, Andrew. ACM SIGOPS Operating Systems Review, 37(5) . 164-177.<br />
<a href="http://www.cl.cam.ac.uk/research/srg/netos/papers/2003-xensosp.pdf">Xen and the Art of Virtualization</a><br />
</li>
<br /><br />
<li>
<b>Bigtable: A Distributed Storage System for Structured Data</b><br />
Chang, Fay;Dean, Jeffrey;Ghemawat, Sanjay;Hsieh, Wilson C;Wallach, Deborah A;Burrows, Mike;Chandra, Tushar;Fikes, Andrew; and Gruber, Robert E. ACM Transactions on Computer Systems (TOCS), 26(2) 2008.<br />
<a href="http://static.googleusercontent.com/media/research.google.com/en/us/archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a><br />
</li>
<br /><br />
<li>
<b>Live Migration of Virtual Machines</b><br />
Clark, Christopher;Fraser, Keir;Hand, Steven;Hansen, Jacob Gorm;Jul, Eric;Limpach, Christian;Pratt, Ian; and Warfield, Andrew. Proceedings of the 2nd conference on Symposium on Networked Systems Design &amp; Implementation-Volume 2. 273-286.<br />
<a href="https://www.usenix.org/legacy/events/nsdi05/tech/full_papers/clark/clark.pdf?q=live-migration-of-virtual-machines">Live Migration of Virtual Machines</a><br />
</li>
<br /><br />
<li>
<b>Cassandra—A Decentralized Structured Storage System</b><br />
Lakshman, Avinash and Malik, Prashant. Operating systems review, 44(2) 2010. 35.<br />
<a href="https://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf">Cassandra: A Decentralized Structured Storage System</a><br />
</li>
<br /><br />
<li>
<b>Dynamo: Amazon&#8217;s Highly Available Key-Value Store</b><br />
DeCandia, Giuseppe;Hastorun, Deniz;Jampani, Madan;Kakulapati, Gunavardhan;Lakshman, Avinash;Pilchin, Alex;Sivasubramanian, Swaminathan;Vosshall, Peter; and Vogels, Werner. ACM SIGOPS Operating Systems Review, 41(6) . 205-220.<br />
<a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo: Amazon&#8217;s Highly Available Key-Value Store</a><br />
</li>
<br /><br />
<li>
<b>Serving Large-scale Batch Computed Data with Project Voldemort</b><br />
Roshan Sumbaly; Jay Kreps; Lei Gao; Alex Feinberg; Chinmay Soman; Sam Shah;<br />
LinkedIn, Usenix<br />
<a href="https://www.usenix.org/legacy/events/fast/tech/full_papers/Sumbaly.pdf">Serving Large-scale Batch Computed Data with Project Voldemort</a><br />
</li>
<br /><br />
<li>
<b>PNUTS: Yahoo!’s Hosted Data Serving Platform</b><br />
Brian F. Cooper, Raghu Ramakrishnan, Utkarsh Srivastava, Adam Silberstein, Philip Bohannon, HansArno Jacobsen, Nick Puz, 
Daniel Weaver and Ramana Yerneni <br />
Yahoo! Research.<br />
<a href="http://www.mpi-sws.org/~druschel/courses/ds/papers/cooper-pnuts.pdf">PNUTS: Yahoo!’s Hosted Data Serving Platform</a><br />
</li>
<br /><br />
<li>
<b>Hive: A Warehousing Solution Over a Map-Reduce Framework</b><br />
Thusoo, Ashish;Sarma, Joydeep Sen;
Jain, Namit;Shao, Zheng;Chakka, Prasad;Anthony, Suresh;Liu, Hao;Wyckoff, Pete; and
Murthy, Raghotham. Proceedings of the VLDB Endowment, 2(2) 2009. 1626-1629. <br />
<a href="http://www.vldb.org/pvldb/2/vldb09-938.pdf">Hive: A Warehousing Solution Over a Map-Reduce Framework</a><br />
</li>
<br /><br />
<li>
<b>MapReduce: Simplified Data Processing on Large Clusters</b><br />
Dean, Jeffrey and Ghemawat, Sanjay. Communications of the ACM, 51(1) 2008. 107-113. <br />
<a href="http://static.googleusercontent.com/media/research.google.com/en/us/archive/mapreduce-osdi04.pdf">MapReduce: Simplified Data Processing on Large Clusters</a><br />
</li>
<br /><br />
<li>
<b>An Analysis of Facebook Photo Caching</b><br />
Qi Huang, Ken Birman, Robbert van Renesse (Cornell University), Wyatt Lloyd (Princeton University), Sanjeev Kumar, Harry C. Li (Facebook Inc.)<br />
<a href="http://www.cs.cornell.edu/~qhuang/papers/sosp_fbanalysis.pdf">An Analysis of Facebook Photo Caching</a><br />
</li>
<br /><br />
<li>
<b>Scaling Memcache at Facebook</b><br />
Rajesh Nishtala, Hans Fugal, Steven Grimm, Marc Kwiatkowski, Herman Lee, Harry C. Li, Ryan McElroy, Mike Paleczny, Daniel Peek, Paul Saab, David Stafford, Tony Tung, and Venkateshwaran Venkataramani, Facebook Inc. NSDI 2013<br />
<a href="https://cs.uwaterloo.ca/~brecht/courses/854-Emerging-2014/readings/key-value/fb-memcached-nsdi-2013.pdf">Scaling Memcache at Facebook</a><br />
</li>
<br /><br />
<li>
<b>Hey, You, Get Off of My Cloud: Exploring Information Leakage in Third-Party Compute Clouds</b><br />
Thomas Ristenpart, Eran Tromer, Hovav Shacham, Stefan Savage<br />
<a href="https://cseweb.ucsd.edu/~hovav/dist/cloudsec.pdf">Hey, You, Get Off of My Cloud: Exploring Information Leakage in Third-Party Compute Clouds</a><br />
</li>
<br /><br />
<li>
<b>Apache Kafka: a Distributed Messaging System for Log Processing</b><br />
Jay Kreps, Neha Narkhede, Jun Rao<br />
<a href="http://research.microsoft.com/en-us/um/people/srikanth/netdb11/netdb11papers/netdb11-final12.pdf">Apache Kafka: a Distributed Messaging System for Log Processing</a><br />
</li>
<br /><br />
<li>
<b>Spark: Cluster Computing with Working Sets</b><br />
Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, Ion Stoica, University of California, Berkeley<br />
<a href="http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf">Spark: Cluster Computing with Working Sets</a><br />
</li>
<br /><br />
<li>
<b>Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</b><br />
Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker, Ion Stoica
University of California, Berkeley<br />
<a href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf">RDD: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</a><br />
</li>
<br /><br />
<li>
<b>Discretized Streams: Fault-Tolerant Streaming Computation at Scale</b><br />
Matei Zaharia, Tathagata Das, Haoyuan Li, Timothy Hunter, Scott Shenker, Ion Stoica, University of California, Berkeley<br />
<a href="https://www.cs.berkeley.edu/~matei/papers/2013/sosp_spark_streaming.pdf">Discretized Streams: Fault-Tolerant Streaming Computation at Scale</a><br />
</li>
<br /><br />
<li>
<b>Shark: SQL and Rich Analytics at Scale</b><br />
Reynold S. Xin, Josh Rosen, Matei Zaharia, Michael J. Franklin, Scott Shenker, Ion Stoica<br />
<a href="https://www.cs.berkeley.edu/~matei/papers/2013/sigmod_shark.pdf">Shark: SQL and Rich Analytics at Scale</a><br />
</li>
</ul>
<br /><br />

</p>

]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[Using Multiple Heat Layers with Google Maps]]></title>
        <link href="http://www.deepmusings.com/blog/2015/01/09/using-multiple-heat-layers-with-google-maps/"/>
        <updated>2015-01-09T18:03:17-08:00</updated>
        <id>http://www.deepmusings.com/blog/2015/01/09/using-multiple-heat-layers-with-google-maps</id>
        <content type="html"><![CDATA[<p>For one of the assignments in Cloud &amp; Big Data course at Columbia University, we had to display a Heat Map Layer showing positive and negative tweets from Twitter.  </p>


<p> Now, using Heatmaps with Google Maps is pretty trivial. Google Maps has pretty decent documentation along with working examples. </p>




<p>But how do we do it when we want to show muliple heat maps working independently on the same Google Map?</p>




<!--more-->




<p> Let&#8217;s take a closer look at the standard Heatmap example first. </p>


<p>Go through the example given <a href="https://developers.google.com/maps/documentation/javascript/examples/layer-heatmap" title="Heatmaps examples">here</a>.</p>

<p>As we see, the following piece of code is responsible for initializing the heat map. It also has a function called changeGradient() which is of interest to us. </p>




<pre>
<code class="javascript">

function initialize() {
  var mapOptions = {
    zoom: 13,
    center: new google.maps.LatLng(37.774546, -122.433523),
    mapTypeId: google.maps.MapTypeId.SATELLITE
  };

  map = new google.maps.Map(document.getElementById('map-canvas'),
      mapOptions);

  var pointArray = new google.maps.MVCArray(taxiData);

  heatmap = new google.maps.visualization.HeatmapLayer({
    data: pointArray
  });

  heatmap.setMap(map);
}


function changeGradient() {
  var gradient = [
    'rgba(0, 255, 255, 0)',
    'rgba(0, 255, 255, 1)',
    'rgba(0, 191, 255, 1)',
    'rgba(0, 127, 255, 1)',
    'rgba(0, 63, 255, 1)',
    'rgba(0, 0, 255, 1)',
    'rgba(0, 0, 223, 1)',
    'rgba(0, 0, 191, 1)',
    'rgba(0, 0, 159, 1)',
    'rgba(0, 0, 127, 1)',
    'rgba(63, 0, 91, 1)',
    'rgba(127, 0, 63, 1)',
    'rgba(191, 0, 31, 1)',
    'rgba(255, 0, 0, 1)'
  ]
  heatmap.set('gradient', heatmap.get('gradient') ? null : gradient);
}
</code> 
</pre>




<p>The changeGradient function changes the default gradient values to anything you specify. If you notice, the color is changing from light blue rgba(0, 255, 255, 0) for low density points to Sharp Red rgba(255, 0, 0, 1) as the density increases. 
</p>




<p>What you can simply do is, initiate two different heatmaps, say heatmapPositive (light blue to dark blue) and heatmapNegative (yellow to red) in case you want to show two different heat maps for the positive and negative sentiment tweets. </p>




<p>If you want sample code for this, you can find it <a href="https://github.com/BhavdeepSethi/cloudBigData/blob/master/twitterMapSource/twitMap.js" title="Github Link" >here</a>.  If you want to see how this looks, you can check out <a href="https://www.youtube.com/watch?v=9Qv7F_43dOk" >this video</a> which shows the default state and then with the dual heat maps. </p>




<p> If you want to do much more, there is this neat library called <a href="http://www.patrick-wied.at/static/heatmapjs/" title="heatmap.js">heatmap.js</a>. It has bunch of features like adding legends and tool tips, customized maps, etc. 
</p>




<p> Hope you found this useful. If you&#8217;re aware of any better libraries out there, then Let me know!</p>



]]></content>
    </entry>
    
    <entry>
        <title type="html"><![CDATA[MariaDB seconds behind issue fixed]]></title>
        <link href="http://www.deepmusings.com/blog/2014/03/10/mariadb-seconds-behind-issue-fixed/"/>
        <updated>2014-03-10T13:28:38-07:00</updated>
        <id>http://www.deepmusings.com/blog/2014/03/10/mariadb-seconds-behind-issue-fixed</id>
        <content type="html"><![CDATA[<p>We (my friend Vishnu and I) had reported a bug with MariaDB 10.0.4. Just got an update that the issue is fixed now! Here are the details about the bug:
We had setup maria 10.0.4 for multisource replication. It was replicating from a Percona 5.5 master. When we started replication, Maria was a day behind its master. It was working fine, but we noticed this phenomenon:
When we execute <code language="sql">show all slaves status</code> consecutively, the &lsquo;seconds_behind_master&rsquo; flips from 0 to XXX and XXX to 0.
Note: XXX is some positive number.</p>

<p>Reference: <a href="https://mariadb.atlassian.net/browse/MDEV-5114">https://mariadb.atlassian.net/browse/MDEV-5114</a></p>

<p>The fix is deployed in MariaDB 10.0.10.</p>
]]></content>
    </entry>
    
</feed>
